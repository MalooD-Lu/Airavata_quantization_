# Airavata Quantization

This project demonstrates how to **load**, **quantize**, and **serve** the [Ai4Bharat/Airavata](https://huggingface.co/ai4bharat/Airavata) Hindi LLM using FastAPI. It also includes a benchmark to evaluate the modelâ€™s latency and throughput using the deployed API.

---

## ðŸ“Œ Features

- âœ… Load **base** and **4-bit quantized** versions of Airavata LLM
- âœ… Deploy using **FastAPI + Uvicorn + ngrok**
- âœ… Measure **latency** and **throughput** using `requests`
- âœ… Full support for **Google Colab + GPU**

---

