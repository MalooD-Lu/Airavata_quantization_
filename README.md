# Airavata Quantization

This project demonstrates how to **load**, **quantize**, and **serve** the [Ai4Bharat/Airavata](https://huggingface.co/ai4bharat/Airavata) Hindi LLM using FastAPI. It also includes a benchmark to evaluate the modelâ€™s latency and throughput using the deployed API.

---

## ðŸ“Œ Features

- âœ… Load **base** and **4-bit quantized** versions of Airavata LLM
- âœ… Deploy using **FastAPI + Uvicorn + ngrok**
- âœ… Measure **latency** and **throughput** using `requests`
- âœ… Full support for **Google Colab + GPU**

---

## ðŸ““ Google Colab Notebook

All the following were implemented in a **single Google Colab notebook**:

- âœ… Quantization using `bitsandbytes`
- âœ… FastAPI setup with working `/generate` endpoint
- âœ… ngrok tunneling for external access
- âœ… Latency and throughput benchmarking

> âš ï¸ **Note**: Make sure to enable GPU via  
> `Runtime > Change runtime type > Hardware accelerator > GPU`.

---
## ðŸ“Š Benchmark Results

| Metric        | Base Model | Quantized Model (4-bit) |
|---------------|------------------------|---------------------------|
| **Latency**   | 2046.86 ms        | 2010 ms           |
| **Throughput**| 0.49 req/sec          | 0.50 req/sec             |
